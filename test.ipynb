{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from glob import glob\n",
    "from time import time\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from Inference import Inference as inf\n",
    "from models import ViT, configs\n",
    "from utils import dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13875, 33) | (8233, 33) | (12443, 33)\n"
     ]
    }
   ],
   "source": [
    "data_loader_training, data_loader_validate, data_loader_test = dataloader.return_dataloaders(batch_size = 1, \n",
    "                                                                                            in_channels = 4, \n",
    "                                                                                            lds_ks = 10,\n",
    "                                                                                            lds_sigma = 8, \n",
    "                                                                                            dw_alpha = 3.9, \n",
    "                                                                                            betha  = 4, \n",
    "                                                                                            reweighting_method = 'lds', \n",
    "                                                                                            resmapling_status = False,\n",
    "                                                                                            exp_name = 't1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 16, 16, 15]) torch.Size([1, 1, 16, 16, 1])\n",
      "[tensor([2]), tensor([3]), tensor([11]), tensor([5])]\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(data_loader_test))   \n",
    "print(sample[\"image\"].shape, sample[\"mask\"].shape)\n",
    "print(sample[\"EmbList\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configs.SRTR_Configs(img_size = 16, patch_size = 8, embed_dim = 1024, mlp_dim = 512, \n",
    "                    in_channels = 4, out_channels = 15, num_heads = 8, num_layers = 6, cond = None,\n",
    "                    Attn_drop = 0.1, Proj_drop = 0.1, PostNorm = False, vis = True, \n",
    "                    kernel_size = [3, 3], dilation = [1, 1], stride  = [1, 1]\n",
    "                    ).call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT.ours(config).to(device)\n",
    "# best_model_name = '/data2/hkaman/DiT/EXPs/EXP_vanilla_mse_01_0001_1024_8_6/best_model_vanilla_mse_01_0001_1024_8_6.pth'\n",
    "# model.load_state_dict(torch.load(best_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$R^2=-1.17$\n",
      "MAE=1.23\n",
      "$RMSE=1.71$\n",
      "$MAPE=0.15$\n"
     ]
    }
   ],
   "source": [
    "pred = model(sample[\"image\"].to(device), sample[\"EmbList\"])\n",
    "\n",
    "r2, mae, rmse, mape, _,_ = inf.regression_metrics(sample[\"mask\"][0, 0, :, :, 0].detach().cpu().numpy(), pred[0, 0, :, :].detach().cpu().numpy())\n",
    "scores = (r'$R^2={:.2f}$' + '\\n' + r'MAE={:.2f}' + '\\n' + r'$RMSE={:.2f}$' + '\\n' + r'$MAPE={:.2f}$').format(\n",
    "    r2, mae, rmse, mape)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import engine\n",
    "ImYiEst = engine.ImbYieldEst(model, \n",
    "                                lr = 0.01, \n",
    "                                wd = 0.0001, \n",
    "                                exp = 'vanilla_mse_01_0001_1024_8_6')\n",
    "\n",
    "_ = ImYiEst.predict(model, data_loader_training, data_loader_validate, data_loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Learning Rate Annealing...\n",
      "Epoch [20/100], Loss: 2.7535, Learning Rate: 0.0038\n",
      "Epoch [40/100], Loss: 2.1590, Learning Rate: 0.0014\n",
      "Epoch [60/100], Loss: 1.9798, Learning Rate: 0.0005\n",
      "Epoch [80/100], Loss: 1.9194, Learning Rate: 0.0002\n",
      "Epoch [100/100], Loss: 1.8983, Learning Rate: 0.0001\n",
      "Epoch [120/100], Loss: 1.8907, Learning Rate: 0.0000\n",
      "Epoch [140/100], Loss: 1.8881, Learning Rate: 0.0000\n",
      "Epoch [160/100], Loss: 1.8871, Learning Rate: 0.0000\n",
      "Epoch [180/100], Loss: 1.8867, Learning Rate: 0.0000\n",
      "Epoch [200/100], Loss: 1.8866, Learning Rate: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Example linear regression model\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.fc = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Generate some random data for training\n",
    "torch.manual_seed(42)\n",
    "x = torch.rand((100, 1))\n",
    "y = 2 * x + 1 + torch.randn((100, 1)) * 0.1  # Adding some noise\n",
    "\n",
    "# Define model, loss function, and initial learning rates\n",
    "model = LinearRegression()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Learning rate for annealing (start high)\n",
    "learning_rate_anneal = 0.01\n",
    "optimizer_anneal = optim.SGD(model.parameters(), lr=learning_rate_anneal)\n",
    "\n",
    "# Learning rate for warm-up (start low)\n",
    "learning_rate_warmup = 0.001\n",
    "optimizer_warmup = optim.SGD(model.parameters(), lr=learning_rate_warmup)\n",
    "\n",
    "# Training loop with learning rate annealing\n",
    "print(\"Training with Learning Rate Annealing...\")\n",
    "for epoch in range(200):\n",
    "    optimizer_anneal.zero_grad()\n",
    "    outputs = model(x)\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer_anneal.step()\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/100], Loss: {loss.item():.4f}, Learning Rate: {learning_rate_anneal:.4f}\")\n",
    "    # Anneal the learning rate (e.g., reduce by a factor)\n",
    "    learning_rate_anneal *= 0.95\n",
    "    for param_group in optimizer_anneal.param_groups:\n",
    "        param_group['lr'] = learning_rate_anneal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with Learning Rate Warm-Up...\n",
      "Epoch [20/100], Loss: 1.5768, Learning Rate: 0.0029\n",
      "Epoch [40/100], Loss: 1.0794, Learning Rate: 0.0049\n",
      "Epoch [60/100], Loss: 0.6160, Learning Rate: 0.0069\n",
      "Epoch [80/100], Loss: 0.3048, Learning Rate: 0.0089\n",
      "Epoch [100/100], Loss: 0.1448, Learning Rate: 0.0109\n",
      "Epoch [120/100], Loss: 0.0793, Learning Rate: 0.0129\n",
      "Epoch [140/100], Loss: 0.0564, Learning Rate: 0.0149\n",
      "Epoch [160/100], Loss: 0.0480, Learning Rate: 0.0169\n",
      "Epoch [180/100], Loss: 0.0434, Learning Rate: 0.0189\n",
      "Epoch [200/100], Loss: 0.0395, Learning Rate: 0.0209\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "# Training loop with learning rate warm-up\n",
    "print(\"\\nTraining with Learning Rate Warm-Up...\")\n",
    "for epoch in range(200):\n",
    "    optimizer_warmup.zero_grad()\n",
    "    outputs = model(x)\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer_warmup.step()\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/100], Loss: {loss.item():.4f}, Learning Rate: {learning_rate_warmup:.4f}\")\n",
    "    # Warm-up the learning rate (e.g., increase linearly)\n",
    "    learning_rate_warmup += 0.0001\n",
    "    for param_group in optimizer_warmup.param_groups:\n",
    "        param_group['lr'] = learning_rate_warmup\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
